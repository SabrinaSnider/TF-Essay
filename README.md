# Overview
This project investigates how the number of neurons in an artifical neural network impacts its prediction accuracy. This idea
boils down to how neural network complexity impacts accuracy and the drawbacks of overfitting and underfitting, if any. To gather
data for this investigation, a simple 3-layer neural network trained on Boston housing data was tested with different amounts 
of neurons. After obtaining data, the trial errors were compared to gain better insight on the project topic.

## Background
Artificial neural networks are a specific type of machine learning algorithm that can effectively model nonlinear and complex datasets. This 
makes them imperative in understanding real-life data, which often exhibits such characteristics. For this reason, artificial neural networks 
have grown to dominate the field of machine learning, being a flexible and powerful tool for evaluating big data. Despite being such a
powerful tool, there is uncertainty in how to structure a neural network to optimize its accuracy. The best structure often if depented on
the type of data being trained on and what level of noise it possesses.

The growing importance of neural networks in computing led me to investigate how the number of neurons in an neural network impact its accuracy.
In particular, I wanted to test whether developing a large and complex neural network would be consistently better, of if such a model was be
over-complicated and erroneous.

## Tools
- Python
- TensorFlow library
- Scikit-Learn library
- Metrics library
